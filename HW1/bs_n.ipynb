{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Use recommended parameters from paper of Adam: \n",
    "            -- https://arxiv.org/abs/1412.6980\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.eps = epsilon\n",
    "        self.iter = 1\n",
    "    def update(self, params, grads):\n",
    "        f_param = params.ravel()\n",
    "        f_grad  = grads.ravel()\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = np.zeros_like(f_param)\n",
    "            self.vs = np.zeros_like(f_param)\n",
    "        for i, (x, dx, m, v) in enumerate(zip(f_param, f_grad, self.ms, self.vs)):    \n",
    "            # Evaluate:\n",
    "            m = self.beta_1*m + (1-self.beta_1)*dx # m_t = b1*m_t-1 + (1-b1)*g\n",
    "            mt = m / (1-self.beta_1**self.iter) # m_t_h = m_t / (1-b1^t)\n",
    "            v = self.beta_2*v + (1-self.beta_2)*(dx**2) # v_t = b2*v_t-1 + (1-b2)*g^2\n",
    "            vt = v / (1-self.beta_2**self.iter) # v_t_h = v_t / (1-b2^t)\n",
    "            \n",
    "            # Update:\n",
    "            f_param[i] -= self.lr * mt / (np.sqrt(vt) + self.eps) # theta = -lr * m_t_h / (sqrt(v_t_h) + eps)\n",
    "            self.ms[i] = m # write m_t to memory (update from m_t-1 to m_t)\n",
    "            self.vs[i] = v # write v_t to memory (update from v_t-1 to v_t)\n",
    "        self.iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(object):\n",
    "    def __init__(self, lr=0.001, decay_rate=0.9, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Ref from CS231n:\n",
    "        http://cs231n.github.io/neural-networks-3\n",
    "        cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "        x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.decay = decay_rate\n",
    "        self.eps = epsilon\n",
    "    def update(self, params, grads):\n",
    "        f_param = params.ravel()\n",
    "        f_grad  = grads.ravel()\n",
    "        if not hasattr(self, 'cache'):\n",
    "            self.cache = np.zeros_like(f_param)\n",
    "        for i, (x, dx, c) in enumerate(zip(f_param, f_grad, self.cache)):    \n",
    "            # Evaluate:\n",
    "            c_t = self.decay * c + (1 - self.decay) * dx**2\n",
    "            \n",
    "            # Update:\n",
    "            f_param[i] -= self.lr * dx / (np.sqrt(c_t) + self.eps) \n",
    "            self.cache[i] = c_t # update cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(x, g_f, iterations=2000, optimizer=Adam(), approximate_gradient=False, f=None, x_eps=1e-8, return_seq=False):\n",
    "    x = x.copy()\n",
    "    if return_seq:\n",
    "        xt = [x]\n",
    "        yt = [f(x)]\n",
    "    for _ in range(iterations):\n",
    "        if approximate_gradient:\n",
    "            grad = (f(x+x_eps) - f(x)) / x_eps \n",
    "        else:\n",
    "            grad = g_f(x)\n",
    "        optimizer.update(x, grad)\n",
    "        if return_seq:\n",
    "            xt.append(x)\n",
    "            yt.append(f(x))\n",
    "    if return_seq:\n",
    "        return x, xt, yt\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic definition of fx funciton\n",
    "# fx = lambda x: x**4 - 3*(x**2) + 2\n",
    "# g_fx = lambda x: 4*x**3 - 6*x\n",
    "def fx(x):\n",
    "    return np.squeeze(x**4 - 3*(x**2) + 2)\n",
    "\n",
    "def g_fx(x):\n",
    "    ret = np.zeros_like(x)\n",
    "    ret[...] = 4*x**3 - 6*x\n",
    "    return ret\n",
    "\n",
    "# Basic definition of rosenbrock function\n",
    "# rosenbrock = lambda x1, x2: 100*(x2-x1)**2 + (1-x1)**2\n",
    "# g_rosenbrock = lambda x1, x2: (202*x1 - 200*x2 - 2, -200*(x1-x2)) # partial_x1, partial_x2\n",
    "def rosenbrock(x):\n",
    "    return np.squeeze(100*(x[...,1]-x[...,0])**2 + (1-x[...,0])**2) # (batch_size)\n",
    "\n",
    "def g_rosenbrock(x):\n",
    "    ret = np.zeros_like(x)\n",
    "    ret[...,0] = 202*x[...,0] - 200*x[...,1] - 2\n",
    "    ret[...,1] = -200*(x[...,0]-x[...,1])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(xs, ys, f):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fx_x:', array([1.53707316]))\n"
     ]
    }
   ],
   "source": [
    "fx_x = np.random.randn(1)\n",
    "print('fx_x:', fx_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rosenbrock_x:', array([0.67353489, 0.84394122]))\n"
     ]
    }
   ],
   "source": [
    "rosenbrock_x = np.random.randn(2) + 1\n",
    "print('rosenbrock_x:', rosenbrock_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Adagrad', 'fx', array([1.22474487]), array(0.49406821), 10000)\n",
      "('Adam', 'fx', array([1.22474487]), array(0.49406821), 10000)\n",
      "('Adagrad', 'rosenbrock', array([0.84548636, 0.84394122]), 3.0104114236778665, 100000)\n",
      "('Adam', 'rosenbrock', array([0.84548636, 0.84394122]), 3.0104114236778665, 100000)\n"
     ]
    }
   ],
   "source": [
    "optimizers = [Adagrad, Adam]\n",
    "funcs = [fx, rosenbrock]\n",
    "grads = [g_fx, g_rosenbrock]\n",
    "xs    = [fx_x, rosenbrock_x]\n",
    "iters = [10000, 100000]\n",
    "for func, x, g_func, iter_ in zip(funcs, xs, grads, iters):\n",
    "    for opt_class in optimizers:\n",
    "        opt = opt_class()\n",
    "        final_x, xs, ys = minimize(x, g_func, iterations=iter_, f=func, return_seq=True)\n",
    "        visualize(xs, ys, func)\n",
    "        print(type(opt).__name__, func.__name__, final_x, func(x), iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
